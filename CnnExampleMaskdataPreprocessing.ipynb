{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e13632d-64cd-4181-a76d-55d1290e050b",
   "metadata": {},
   "source": [
    "# Preprocessing of Mask data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9087f6c3-bc49-4c48-9cbb-06f0e6cc0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\athira a r\\anaconda3\\envs\\deepnn\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\athira a r\\anaconda3\\envs\\deepnn\\lib\\site-packages (from opencv-python) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d052d3d-8a1d-40bf-bf36-1ef91cab9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='Mask data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e84c3148-a03c-464b-8adc-d0bdba08f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=os.listdir(data_path)                # List of subdirectories (categories),ie, here 2 folders with mask and without mask\n",
    "labels=[i for i in range(len(categories))]       # Numerical labels for each category,ie,len(catagories)=2. range(len(catagories))= range(2)=0,1.ie, here assign values 0&1 to without mask and with mask accordingly\n",
    "label_dict=dict(zip(categories,labels))         # Dictionary mapping category names to numerical labels.ie,here for mask data ,label 1, and without mask label 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "324671f6-f6aa-4d1a-902e-048c6edb8f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'without_mask': 0, 'with_mask': 1}\n"
     ]
    }
   ],
   "source": [
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13284520-1bc0-475d-a4dc-bfd53991f3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['without_mask', 'with_mask']\n"
     ]
    }
   ],
   "source": [
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ec58c49-8275-4f72-a67a-9635975352f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6b42333-6810-4e23-a7cd-e912bf35e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=100\n",
    "data=[]          # create a empty list data\n",
    "target=[]        #create a empty list target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6fb4380-f4f2-4fd1-8e67-3dc9df89aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    folder_path=os.path.join(data_path,category)   #join the specified paths,data_path & catagory. ie here joins mask data + without mask & mask data+ with mask\n",
    "    img_names=os.listdir(folder_path)              #here creating alist, ie,every image names in 2 folders(without mask &with mask)\n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)     # joins mask data+ with out mask+ image name (all image in that folder) & mask data+ withmask+all images\n",
    "        img=cv2.imread(img_path)                          # to read an image from file specified path.\n",
    "        try:\n",
    "            gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)     #convert image to grayscale\n",
    "            resized=cv2.resize(gray,(img_size,img_size))  #resizing the grayscale into 100*100,#since we need a fixed common size for all the images\n",
    "            data.append(resized)                          #append the resized data to previously ceated empty list 'data'\n",
    "            target.append(label_dict[category])           #appending the image and the label (categorized) into the list (dataset)\n",
    "        except Exception as e:\n",
    "            print('Exception:',e)                         #if there is any exception will be printed here and pass to the next image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "331d54f8-aed8-4ac1-a95d-47938c0063dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data=np.array(data)/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc37d981-be2b-49c1-bff9-c0c2b255aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the pixel values of the images in the NumPy array.Divides each element in the NumPy array by 255.0.Pixel values are typically in the range of 0 to 255 (for an 8-bit image), and dividing by 255 scales the values to be in the range of 0 to 1.0. Normalizing to this range can make training neural networks more stable and can help convergence during the optimization process.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fb17e8a-918b-405d-8c7d-d712b1fbfb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1376, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "508e2a79-347f-4076-ae40-1077a4558612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1376) Â number of samples or images, (100, 100) represent the height and width of each image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "219d254d-9baf-48de-bfa9-2516e68f932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.reshape(data,(data.shape[0],img_size,img_size,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "014101e5-38ba-4dce-9499-2e42ec3e725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your original shape was (1376, 100, 100) (1376 images, each of size 100x100 pixels), after reshaping, it becomes (1376, 100, 100, 1), indicating that there is one channel for each image. 1 is the new dimension added, and it typically represents the number of channels. In this case, it is used for grayscale images, where there is only one channel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a85e5f0-9e3e-4d19-a4aa-19957fd61ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=np.array(target)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8dab90d-efd8-4639-9815-de9d87f12d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "new_target=np_utils.to_categorical(target)\n",
    "print(new_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9180ad6-ed43-4715-94d4-e6473265aea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9c04252-e87a-4a70-95e2-5ca50d3c48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data',data)\n",
    "np.save('target',new_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed49f76-be77-46e0-a2c9-8f9950bfd157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ee9eb-d568-4240-b8b2-2ff3949b85dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283f022-43b4-43d3-9846-a3b100aa22e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
